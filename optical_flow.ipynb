{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd875c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8aaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4'\n",
    "filepath_check = './checkpoint/'\n",
    "input_dim = 500\n",
    "maxCorners = 100\n",
    "frames = 30\n",
    "net_len = 100\n",
    "units = 100\n",
    "output_size = 2\n",
    "classes_list = [\"stop\", \"go\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86f99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal = ['./horizontal/Video-7-36-30-868-22-03-2022/Video-7-36-30-868 (S1WK)',\n",
    "            #'./horizontal/Video-10-6-45-47-30-03-2022/Video-10-6-45-47 (J57V)',\n",
    "            './horizontal/Video-15-43-13-884-22-03-2022/Video-15-43-13-884 (J4NP)',\n",
    "            './horizontal/Video-10-25-45-905-31-03-2022/Video-10-25-45-905 (3W3K)',\n",
    "            './horizontal/Video-10-56-13-801-31-03-2022/Video-10-56-13-801 (X0N9)',            \n",
    "            './horizontal/Video-11-19-18-132-30-03-2022/Video-11-19-18-132 (SH2G)',\n",
    "            './horizontal/Video-12-51-53-673-27-03-2021/Video-12-51-53-673 (NKXJ)',\n",
    "            './horizontal/Video-15-26-20-118-29-03-2022/Video-15-26-20-118 (5FLZ)',\n",
    "            './horizontal/Video-15-32-33-709-31-03-2022/Video-15-32-33-709 (7BAA)',\n",
    "            './horizontal/Video-15-41-30-94-29-03-2022/Video-15-41-30-94 (PH33)',\n",
    "            #'./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC)',\n",
    "            './horizontal/Video-15-49-30-274-29-03-2022/Video-15-49-30-274 (JP92)',\n",
    "            './horizontal/Video-15-8-58-789-27-03-2022/Video-15-8-58-789 (SSQM)',\n",
    "            './horizontal/Video-17-14-15-118-06-03-2021/Video-17-14-15-118 (PE9P)',\n",
    "            './horizontal/Video-17-34-45-652-25-03-2021/Video-17-34-45-652 (BK89)',\n",
    "            './horizontal/Video-17-44-40-74-06-03-2021/Video-17-44-40-74 (CLNF)',\n",
    "            './horizontal/Video-7-51-44-763-22-03-2022/Video-7-51-44-763 (1QN6)',\n",
    "            './horizontal/Video-8-6-54-707-22-03-2022/Video-8-6-54-707 (PKAY)']\n",
    "\n",
    "vertical = ['./vertical/Video-10-15-12-812-14-11-2020/Video-10-15-12-812 (2Y5M)',\n",
    "           './vertical/Video-10-30-21-575-29-09-2021/Video-10-30-21-575 (UOM8)',\n",
    "           './vertical/Video-11-21-2-419-13-09-2021/Video-11-21-2-419 (KVYG)',\n",
    "           './vertical/Video-11-21-6-412-28-12-2020/Video-11-21-6-412 (JQQM)',\n",
    "           './vertical/Video-11-36-12-363-28-09-2021/Video-11-36-12-363 (8119)',\n",
    "           './vertical/Video-11-36-16-421-28-12-2020/Video-11-36-16-421 (PFAQ)',\n",
    "           './vertical/Video-11-5-49-336-13-09-2021/Video-11-5-49-336 (CZPL)',\n",
    "           './vertical/Video-12-23-51-640-02-10-2021/Video-12-23-51-640 (WLWK)',\n",
    "           './vertical/Video-12-39-1-595-02-10-2021/Video-12-39-1-595 (QDUT)',\n",
    "           './vertical/Video-12-44-36-682-24-09-2021/Video-12-44-36-682 (ATO6)',\n",
    "           './vertical/Video-13-16-32-822-10-09-2021/Video-13-16-32-822 (UQ1Q)',\n",
    "           './vertical/Video-13-20-30-695-15-11-2021/Video-13-20-30-695 (VLFO)',\n",
    "           './vertical/Video-13-31-46-773-10-09-2021/Video-13-31-46-773 (MXM7)',\n",
    "           './vertical/Video-16-16-44-458-17-11-2020/Video-16-16-44-458 (LCRL)',\n",
    "           './vertical/Video-16-36-42-174-10-11-2020/Video-16-36-42-174 (IXTE)',\n",
    "           './vertical/Video-16-47-10-870-17-11-2020/Video-16-47-10-870 (YFCN)',\n",
    "           './vertical/Video-17-17-30-919-17-11-2020/Video-17-17-30-919 (ID6X)',\n",
    "           './vertical/Video-17-32-40-752-17-11-2020/Video-17-32-40-752 (E575)',\n",
    "           './vertical/Video-17-47-40-713-02-10-2021/Video-17-47-40-713 (E7IK)',\n",
    "           './vertical/Video-20-21-59-727-24-09-2021/Video-20-21-59-727 (S04R)',\n",
    "           './vertical/Video-20-45-56-152-24-10-2021/Video-20-45-56-152 (6M38)']\n",
    "\n",
    "random.shuffle(horizontal)\n",
    "random.shuffle(vertical)\n",
    "\n",
    "videopath = horizontal + vertical\n",
    "\n",
    "random.shuffle(videopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e390c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def six_or_else(a):\n",
    "    if a[1]==6:\n",
    "        return [1,a[2]]\n",
    "    else:\n",
    "        return [0,a[2]]\n",
    "\n",
    "def mode_periods(df):\n",
    "    last_mode = -1\n",
    "    mode_time = []\n",
    "    mode_time.append([1 if df.iloc[0][0] == 6 else 0 , '00:00:00.0000000']) \n",
    "    for row in df.itertuples():  \n",
    "        if row[1] != last_mode:   \n",
    "            mode_time.append(six_or_else(row))\n",
    "        last_mode = row[1]\n",
    "    mode_time.append([1 if df.iloc[-1][0] == 6 else 0 , '1'+df.iloc[-1][1][1:] ] ) \n",
    "    \n",
    "    return mode_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8c233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RNN(units, input_dim, output_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(units, return_sequences=True, input_shape=(None, 2)))\n",
    "    model.add(layers.GRU(256))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(output_size))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a63535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(filepath):\n",
    "    df = pd.read_csv(filepath[:-4]+'.csv')[['activityMode','timestamp']]\n",
    "    mode_time = mode_periods(df)\n",
    "    \n",
    "    # Read the video \n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "    \n",
    "    # Parameters for ShiTomasi corner detection\n",
    "    feature_params = dict(maxCorners = maxCorners,\n",
    "                       qualityLevel = 0.07,\n",
    "                       minDistance = 10,\n",
    "                       blockSize = 5 )\n",
    "\n",
    "    # Parameters for Lucas Kanade optical flow\n",
    "    lk_params = dict(winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    net = []\n",
    "    label_list = []\n",
    "    i_frame = 0\n",
    "    j_frame = 0\n",
    "    while True:       \n",
    "        ret, old_frame = cap.read()\n",
    "        if not ret:\n",
    "                break\n",
    "        old_frame = cv2.resize(old_frame, (input_dim, input_dim))\n",
    "        height, width, _ = old_frame.shape \n",
    "        part_width = round(width * 0.4)\n",
    "        part_height = round(height * 0.4)\n",
    "        \n",
    "        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "        cv2.rectangle(old_gray, (part_width, part_height), \n",
    "                          (width - part_width, height - part_height), 0, -1)\n",
    "        # Create a mask image for drawing purposes\n",
    "        mask = np.zeros_like(old_frame)\n",
    "        \n",
    "        while True:\n",
    "            # Read new frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            sec = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            res = datetime.timedelta(seconds = sec/1000)\n",
    "            for i in range(len(mode_time)-1): \n",
    "                if (dt.strptime(mode_time[i][1][:-8], \"%H:%M:%S\") - datetime.datetime(1900, 1, 1) <= res) and\\\n",
    "                    (dt.strptime(mode_time[i+1][1][:-8], \"%H:%M:%S\") - datetime.datetime(1900, 1, 1) >= res):\n",
    "                    label = [mode_time[i][0]]                \n",
    "                    break  \n",
    "                    \n",
    "            if (len(label_list)!=0)and(label == label_list[-1])and(j_frame>maxCorners):\n",
    "                continue \n",
    "                \n",
    "            frame = cv2.resize(frame, (input_dim, input_dim))\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.rectangle(old_gray, (part_width, part_height), \n",
    "                          (width - part_width, height - part_height), 0, -1)\n",
    "            # Calculate Optical Flow\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "                old_gray, frame_gray, p0, None, **lk_params\n",
    "            )\n",
    "\n",
    "            # Select good points\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "            \n",
    "            new_net = good_new - good_old\n",
    "            #print(new_net)\n",
    "            while new_net.shape[0] < maxCorners:                \n",
    "                new_net = np.append(new_net, [[0,0]], axis=0)                  \n",
    "            new_net = np.array(new_net)\n",
    "            new_net[np.lexsort((new_net[:,1],new_net[:,0]))]\n",
    "            \n",
    "            i_frame += 1\n",
    "            \n",
    "            if True:#(len(label_list)==0):\n",
    "                j_frame += 1\n",
    "                net.append(new_net) \n",
    "                label_list.append(label)\n",
    "                # Update the previous frame and previous points\n",
    "                old_gray = frame_gray.copy()\n",
    "                p0 = good_new.reshape(-1, 1, 2)\n",
    "                \n",
    "                #if j_frame >= 100:\n",
    "                   # break\n",
    "                continue\n",
    "            \n",
    "            if ((label == label_list[-1]) and (j_frame<=frames)):\n",
    "                j_frame += 1\n",
    "                net.append(new_net) \n",
    "                label_list.append(label)\n",
    "                # Update the previous frame and previous points\n",
    "                old_gray = frame_gray.copy()\n",
    "                p0 = good_new.reshape(-1, 1, 2)\n",
    "                if j_frame >= frames:\n",
    "                    break\n",
    "                continue           \n",
    "                        \n",
    "            if (label != label_list[-1]):\n",
    "                j_frame = 0\n",
    "                net.append(new_net) \n",
    "                label_list.append(label)\n",
    "                # Update the previous frame and previous points\n",
    "                old_gray = frame_gray.copy()\n",
    "                p0 = good_new.reshape(-1, 1, 2)\n",
    "                break\n",
    "                \n",
    "    return np.array(net)/input_dim, np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01d61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lucas_kanade_method_orig(video_path, callbacks, val_x, val_y):\n",
    "    df = pd.read_csv(video_path[:-4]+'.csv')[['activityMode','timestamp']]\n",
    "    mode_time = mode_periods(df)\n",
    "    \n",
    "    model = get_RNN(units, input_dim, output_size)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=\"sgd\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Read the video \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "    \n",
    "    # Parameters for ShiTomasi corner detection\n",
    "    feature_params = dict(maxCorners = maxCorners,\n",
    "                       qualityLevel = 0.07,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "    # Parameters for Lucas Kanade optical flow\n",
    "    lk_params = dict(winSize  = (15, 15),\n",
    "                  maxLevel = 3,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    # Create random colors\n",
    "    color = np.random.randint(0, 255, (500, 3))\n",
    "    predicted_class_name = ' '\n",
    "    while True:       \n",
    "        start = time.time()\n",
    "        # Take first frame and find corners in it\n",
    "        ret, old_frame = cap.read()\n",
    "        old_frame = cv2.resize(old_frame, (input_dim, input_dim))\n",
    "        height, width, _ = old_frame.shape \n",
    "        part_width = round(width * 0.3)\n",
    "        part_height = round(height * 0.3)\n",
    "        \n",
    "        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        cv2.rectangle(old_gray, (part_width, part_height), \n",
    "                          (width - part_width, height - part_height), 0, -1)\n",
    "        \n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "        # Create a mask image for drawing purposes\n",
    "        mask = np.zeros_like(old_frame)\n",
    "        net = []\n",
    "        label_list = []\n",
    "        \n",
    "        while True:\n",
    "            # Read new frame\n",
    "            ret, frame = cap.read()\n",
    "            frame = cv2.resize(frame, (input_dim, input_dim))\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            cv2.rectangle(frame, (part_width, part_height), \n",
    "                          (width - part_width, height - part_height), 0, -1)\n",
    "            \n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # Calculate Optical Flow\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "                old_gray, frame_gray, p0, None, **lk_params\n",
    "            )\n",
    "\n",
    "            # Select good points\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "            \n",
    "            new_net = good_new - good_old\n",
    "            #print(new_net)\n",
    "            while new_net.shape[0] < maxCorners:                \n",
    "                new_net = np.append(new_net, [[0,0]], axis=0)                  \n",
    "            new_net = np.array(new_net)\n",
    "            new_net[np.lexsort((new_net[:,1],new_net[:,0]))]   \n",
    "            net.append(new_net)\n",
    "            \n",
    "            # Draw the tracks\n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "                frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "\n",
    "            sec = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            res = datetime.timedelta(seconds = sec/1000)\n",
    "            for i in range(len(mode_time)-1): \n",
    "                if (dt.strptime(mode_time[i][1][:-8], \"%H:%M:%S\") - datetime.datetime(1900, 1, 1) <= res) and\\\n",
    "                    (dt.strptime(mode_time[i+1][1][:-8], \"%H:%M:%S\") - datetime.datetime(1900, 1, 1) >= res):\n",
    "                    label = [mode_time[i][0]]                \n",
    "                    break  \n",
    "            \n",
    "            label_list.append(label)                \n",
    "                \n",
    "            # Display the demo\n",
    "            img = cv2.add(frame, mask)\n",
    "            cv2.putText(img, predicted_class_name, \n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"frame\", img)\n",
    "            k = cv2.waitKey(25) & 0xFF\n",
    "            if k == 27:\n",
    "                break\n",
    "\n",
    "            # Update the previous frame and previous points\n",
    "            old_gray = frame_gray.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "            if np.array(net).shape[0] >= frames:\n",
    "                net = np.array(net) / input_dim\n",
    "                #history = model.fit(\n",
    "                    #net, np.array(label_list), batch_size=20, epochs=1,\n",
    "                    #callbacks=callbacks,\n",
    "                    #validation_data  = (val_x, val_y) \n",
    "                #)\n",
    "                predicted_labels_probabilities = model.predict(net)[0]\n",
    "                predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "        \n",
    "                # Accessing The Class Name using predicted label.\n",
    "                predicted_class_name = classes_list[predicted_label]\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d732dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 100, 2) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "val_x, val_y = data('./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4')\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1d3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath_check, monitor=\"val_accuracy\",\n",
    "        save_best_only=True, verbose=1,\n",
    "        mode='max')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a33af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#if len(mode_time)<=4:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#continue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x, y \u001b[38;5;241m=\u001b[39m data(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adding Early Stopping Callback\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15,\n",
    "                                        mode = 'max', restore_best_weights = True)\n",
    "\n",
    "model = get_RNN(units, input_dim, output_size)\n",
    "\n",
    "model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=\"sgd\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "for i in videopath:    \n",
    "    df = pd.read_csv(i+'.csv')[['activityMode','timestamp']]\n",
    "    mode_time = mode_periods(df)\n",
    "    \n",
    "    #if len(mode_time)<=4:\n",
    "        #continue\n",
    "    \n",
    "    x, y = data(i + '.mp4')   \n",
    "      \n",
    "    model.fit(\n",
    "            np.array(x), np.array(y), batch_size=20, epochs=3,\n",
    "            callbacks=[early_stopping_callback],\n",
    "            validation_data  = (val_x, val_y) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "307a200f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6/6 [==============================] - 3s 466ms/step - loss: 0.6859 - accuracy: 0.7723 - val_loss: 0.6452 - val_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6229 - accuracy: 1.0000 - val_loss: 0.5739 - val_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 0.5505 - accuracy: 1.0000 - val_loss: 0.4976 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 0.4767 - accuracy: 1.0000 - val_loss: 0.4280 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 0.4099 - accuracy: 1.0000 - val_loss: 0.3688 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 0.3527 - accuracy: 1.0000 - val_loss: 0.3173 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.3063 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.2635 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.2303 - accuracy: 1.0000 - val_loss: 0.2076 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 0.2016 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 0.1773 - accuracy: 1.0000 - val_loss: 0.1606 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 0.1572 - accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 0.1405 - accuracy: 1.0000 - val_loss: 0.1270 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.1141 - accuracy: 1.0000 - val_loss: 0.1022 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b5ae08370>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "            val_x, val_y, batch_size=20, epochs=15,\n",
    "            callbacks=[early_stopping_callback],\n",
    "            validation_data  = (val_x, val_y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c929c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlucas_kanade_method_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mlucas_kanade_method_orig\u001b[0;34m(video_path, callbacks, val_x, val_y)\u001b[0m\n\u001b[1;32m     99\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(img, predicted_class_name, \n\u001b[1;32m    100\u001b[0m             (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    101\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m, img)\n\u001b[0;32m--> 102\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucas_kanade_method_orig('./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4',\n",
    "                         callbacks, val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6046c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
