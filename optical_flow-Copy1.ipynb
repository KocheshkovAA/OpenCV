{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd875c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Conv2D, Flatten\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8aaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4'\n",
    "filepath_check = './checkpoint/'\n",
    "input_dim = 500\n",
    "maxCorners = 500\n",
    "frames = 10\n",
    "net_len = 1000\n",
    "units = 100\n",
    "output_size = 2\n",
    "classes_list = [\"stop\", \"move\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86f99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal = ['./horizontal/Video-7-36-30-868-22-03-2022/Video-7-36-30-868 (S1WK)',\n",
    "            './horizontal/Video-10-6-45-47-30-03-2022/Video-10-6-45-47 (J57V)',\n",
    "            './horizontal/Video-15-43-13-884-22-03-2022/Video-15-43-13-884 (J4NP)',\n",
    "            './horizontal/Video-10-25-45-905-31-03-2022/Video-10-25-45-905 (3W3K)',\n",
    "            './horizontal/Video-10-56-13-801-31-03-2022/Video-10-56-13-801 (X0N9)',            \n",
    "            './horizontal/Video-11-19-18-132-30-03-2022/Video-11-19-18-132 (SH2G)',\n",
    "            './horizontal/Video-12-51-53-673-27-03-2021/Video-12-51-53-673 (NKXJ)',\n",
    "            './horizontal/Video-15-26-20-118-29-03-2022/Video-15-26-20-118 (5FLZ)',\n",
    "            './horizontal/Video-15-32-33-709-31-03-2022/Video-15-32-33-709 (7BAA)',\n",
    "            './horizontal/Video-15-41-30-94-29-03-2022/Video-15-41-30-94 (PH33)',\n",
    "            #'./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC)',\n",
    "            './horizontal/Video-15-49-30-274-29-03-2022/Video-15-49-30-274 (JP92)',\n",
    "            './horizontal/Video-15-8-58-789-27-03-2022/Video-15-8-58-789 (SSQM)',\n",
    "            './horizontal/Video-17-14-15-118-06-03-2021/Video-17-14-15-118 (PE9P)',\n",
    "            './horizontal/Video-17-34-45-652-25-03-2021/Video-17-34-45-652 (BK89)',\n",
    "            './horizontal/Video-17-44-40-74-06-03-2021/Video-17-44-40-74 (CLNF)',\n",
    "            './horizontal/Video-7-51-44-763-22-03-2022/Video-7-51-44-763 (1QN6)',\n",
    "            './horizontal/Video-8-6-54-707-22-03-2022/Video-8-6-54-707 (PKAY)']\n",
    "\n",
    "\n",
    "vertical = [#'./vertical/Video-10-15-12-812-14-11-2020/Video-10-15-12-812 (2Y5M)',\n",
    "           './vertical/Video-10-30-21-575-29-09-2021/Video-10-30-21-575 (UOM8)',\n",
    "           #'./vertical/Video-11-21-2-419-13-09-2021/Video-11-21-2-419 (KVYG)',\n",
    "           './vertical/Video-11-21-6-412-28-12-2020/Video-11-21-6-412 (JQQM)',\n",
    "           './vertical/Video-11-36-12-363-28-09-2021/Video-11-36-12-363 (8119)',\n",
    "           './vertical/Video-11-36-16-421-28-12-2020/Video-11-36-16-421 (PFAQ)',\n",
    "           #'./vertical/Video-11-5-49-336-13-09-2021/Video-11-5-49-336 (CZPL)',\n",
    "           './vertical/Video-12-23-51-640-02-10-2021/Video-12-23-51-640 (WLWK)',\n",
    "           #'./vertical/Video-12-39-1-595-02-10-2021/Video-12-39-1-595 (QDUT)',\n",
    "           #'./vertical/Video-12-44-36-682-24-09-2021/Video-12-44-36-682 (ATO6)',\n",
    "           './vertical/Video-13-16-32-822-10-09-2021/Video-13-16-32-822 (UQ1Q)',\n",
    "           './vertical/Video-13-20-30-695-15-11-2021/Video-13-20-30-695 (VLFO)',\n",
    "           #'./vertical/Video-13-31-46-773-10-09-2021/Video-13-31-46-773 (MXM7)',\n",
    "           #'./vertical/Video-16-16-44-458-17-11-2020/Video-16-16-44-458 (LCRL)',\n",
    "           './vertical/Video-16-36-42-174-10-11-2020/Video-16-36-42-174 (IXTE)',\n",
    "           './vertical/Video-16-47-10-870-17-11-2020/Video-16-47-10-870 (YFCN)',\n",
    "           #'./vertical/Video-17-17-30-919-17-11-2020/Video-17-17-30-919 (ID6X)',\n",
    "           #'./vertical/Video-17-32-40-752-17-11-2020/Video-17-32-40-752 (E575)',\n",
    "           #'./vertical/Video-17-47-40-713-02-10-2021/Video-17-47-40-713 (E7IK)',\n",
    "           './vertical/Video-20-21-59-727-24-09-2021/Video-20-21-59-727 (S04R)',\n",
    "           './vertical/Video-20-45-56-152-24-10-2021/Video-20-45-56-152 (6M38)']\n",
    "\n",
    "random.shuffle(horizontal)\n",
    "random.shuffle(vertical)\n",
    "\n",
    "videopath = horizontal# + vertical\n",
    "\n",
    "random.shuffle(videopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1d3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath_check, monitor=\"val_accuracy\",\n",
    "        save_best_only=True, verbose=1,\n",
    "        mode='max')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be5ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time(video_path, model, callbacks):  \n",
    "    # Read the video \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "    # Parameters for ShiTomasi corner detection\n",
    "    feature_params = dict(maxCorners = maxCorners,\n",
    "                       qualityLevel = 0.07,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7)\n",
    "\n",
    "    # Parameters for Lucas Kanade optical flow\n",
    "    lk_params = dict(winSize  = (15, 15),\n",
    "                  maxLevel = 3,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    color = np.random.randint(0, 255, (500, 3))\n",
    "    plt = []\n",
    "    labels = []\n",
    "    median_class_name = ' '\n",
    "    network_class_name = ' '\n",
    "    \n",
    "    while True:       \n",
    "        # Take first frame and find corners in it\n",
    "        ret, old_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        old_frame = cv2.resize(old_frame, (input_dim, input_dim))        \n",
    "        \n",
    "        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)        \n",
    "                \n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "        # Create a mask image for drawing purposes\n",
    "        mask = np.zeros_like(old_frame)        \n",
    "        \n",
    "        median_list_l = []\n",
    "        median_list_r = []\n",
    "        network_frame = []\n",
    "        model_data = []\n",
    "        while True:\n",
    "            # Read new frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (input_dim, input_dim))\n",
    "            \n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # Calculate Optical Flow\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "                old_gray, frame_gray, p0, None, **lk_params\n",
    "            )\n",
    "            \n",
    "            if p1 is None:\n",
    "                break\n",
    "\n",
    "            # Select good points\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "            #print(good_new)\n",
    "            new_net_l = np.array([])\n",
    "            new_net_r = np.array([])\n",
    "            for i in range(len(good_new)):\n",
    "                if good_new[i][0] < input_dim/2:\n",
    "                    if len(new_net_l) == 0:\n",
    "                        new_net_l = np.array([good_new[i] - good_old[i]])\n",
    "                    else:\n",
    "                        new_net_l = np.append(new_net_l, [good_new[i] - good_old[i]], axis=0)\n",
    "                else:\n",
    "                    if len(new_net_r) == 0:\n",
    "                        new_net_r = np.array([good_new[i] - good_old[i]])\n",
    "                    else:\n",
    "                        new_net_r = np.append(new_net_r, [good_new[i] - good_old[i]], axis=0)\n",
    "                       \n",
    "            # Draw the tracks\n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                #mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "                #frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    " \n",
    "            mask_med = np.zeros_like(old_frame)\n",
    "            if new_net_l.shape[0] != 0:\n",
    "                median_frame_x_l = input_dim/4 + np.median(new_net_l[:, 0])*80  \n",
    "                median_frame_y_l = input_dim/2 + np.median(new_net_l[:, 1])*80\n",
    "                mask_med = cv2.line(mask_med, (int(input_dim/4), int(input_dim/2)),\n",
    "                                (int(median_frame_x_l), int(median_frame_y_l)), (0, 0, 255), 3)\n",
    "                median_frame_l_x = np.median(np.absolute(new_net_l[:, 0]))\n",
    "                median_frame_l_y = np.median(np.absolute(new_net_l[:, 1]))\n",
    "                median_list_l.append([median_frame_l_x,median_frame_l_y])\n",
    "            \n",
    "            if new_net_r.shape[0] != 0:\n",
    "                median_frame_x_r = 3*input_dim/4 + np.median(new_net_r[:, 0])*80  \n",
    "                median_frame_y_r = input_dim/2 + np.median(new_net_r[:, 1])*80\n",
    "                mask_med = cv2.line(mask_med, (int(3*input_dim/4), int(input_dim/2)),\n",
    "                                (int(median_frame_x_r), int(median_frame_y_r)), (0, 0, 255), 3)        \n",
    "                median_frame_r_x = np.median(np.absolute(new_net_r[:, 0]))\n",
    "                median_frame_r_y = np.median(np.absolute(new_net_r[:, 1]))\n",
    "                median_list_r.append([median_frame_r_x,median_frame_r_y])\n",
    "                \n",
    "            img = cv2.add(frame, mask_med)\n",
    "            #img = cv2.add(img, mask_med)\n",
    "            \n",
    "            model_data.append(np.array([median_frame_l_x,median_frame_r_x, median_frame_l_y, median_frame_r_y])/input_dim)\n",
    "\n",
    "            \n",
    "            cv2.putText(img, median_class_name, \n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.putText(img, network_class_name + '(network)', \n",
    "                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                        \n",
    "            cv2.imshow(\"frame\", img)\n",
    "            k = cv2.waitKey(15) & 0xFF\n",
    "            if k == 27:\n",
    "                break\n",
    "            plt.append(np.array([median_frame_l_x,median_frame_r_x, median_frame_l_y, median_frame_r_y])/input_dim)  \n",
    "            \n",
    "            # Update the previous frame and previous points\n",
    "            old_gray = frame_gray.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "            if np.array(median_list_r).shape[0] >= frames:                \n",
    "                median_l = np.median(median_list_l) \n",
    "                if median_l >= 0.09 and median_l <= 30:\n",
    "                    median_class_name_l = 1\n",
    "                else:\n",
    "                    median_class_name_l = 0\n",
    "                                                    \n",
    "                median_r = np.median(median_list_r) \n",
    "                if median_r >= 0.09 and median_r <= 30:\n",
    "                    median_class_name_r = 1\n",
    "                else:\n",
    "                    median_class_name_r = 0\n",
    "                    \n",
    "                label = 0   \n",
    "                if median_class_name_r == 1 and median_class_name_l == 1:\n",
    "                    median_class_name = classes_list[1]\n",
    "                    label = 1\n",
    "                else:\n",
    "                    median_class_name = classes_list[0]\n",
    "                    label = 0\n",
    "                    \n",
    "                predicted_labels_probabilities = model.predict(np.array(model_data),callbacks=callbacks)\n",
    "                network_class_name = classes_list[np.argmax([np.median(np.array(predicted_labels_probabilities)[:,0]),\n",
    "                                                            np.median(np.array(predicted_labels_probabilities)[:,1])])]\n",
    "                \n",
    "                for i in range(frames):\n",
    "                    labels.append(np.array([label]))   \n",
    "                    \n",
    "                    \n",
    "                break\n",
    "    return plt, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604bb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Возвращает данные о движении пикселей, состояния с порога и состояния с нейронной сети \n",
    "def data_model(video_path, model, callbacks):  \n",
    "    # Read the video \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "    # Parameters for ShiTomasi corner detection\n",
    "    feature_params = dict(maxCorners = maxCorners,\n",
    "                       qualityLevel = 0.07,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "    # Parameters for Lucas Kanade optical flow\n",
    "    lk_params = dict(winSize  = (15, 15),\n",
    "                  maxLevel = 3,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "\n",
    "    plt = []\n",
    "    labels = []\n",
    "    model_labels = [] \n",
    "    network_class_name = 0 \n",
    "    label = 0\n",
    "    while True:       \n",
    "        # Take first frame and find corners in it\n",
    "        ret, old_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        old_frame = cv2.resize(old_frame, (input_dim, input_dim))        \n",
    "        \n",
    "        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)        \n",
    "                \n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "        # Create a mask image for drawing purposes\n",
    "        mask = np.zeros_like(old_frame)        \n",
    "        \n",
    "        median_list_l = []\n",
    "        median_list_r = []\n",
    "        network_frame = []\n",
    "        model_data = []\n",
    "        while True:\n",
    "            # Read new frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (input_dim, input_dim))\n",
    "            \n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # Calculate Optical Flow\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "                old_gray, frame_gray, p0, None, **lk_params\n",
    "            )\n",
    "            \n",
    "            if p1 is None:\n",
    "                break\n",
    "\n",
    "            # Select good points\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "            \n",
    "            new_net_l = np.array([])\n",
    "            new_net_r = np.array([])\n",
    "            for i in range(len(good_new)):\n",
    "                if good_new[i][0] < input_dim/2:\n",
    "                    if len(new_net_l) == 0:\n",
    "                        new_net_l = np.array([good_new[i] - good_old[i]])\n",
    "                    else:\n",
    "                        new_net_l = np.append(new_net_l, [good_new[i] - good_old[i]], axis=0)\n",
    "                else:\n",
    "                    if len(new_net_r) == 0:\n",
    "                        new_net_r = np.array([good_new[i] - good_old[i]])\n",
    "                    else:\n",
    "                        new_net_r = np.append(new_net_r, [good_new[i] - good_old[i]], axis=0)\n",
    "                       \n",
    " \n",
    "            mask_med = np.zeros_like(old_frame)\n",
    "            if new_net_l.shape[0] != 0:\n",
    "                median_frame_l_x = np.median(np.absolute(new_net_l[:, 0]))\n",
    "                median_frame_l_y = np.median(np.absolute(new_net_l[:, 1]))\n",
    "                median_list_l.append([median_frame_l_x,median_frame_l_y])\n",
    "            \n",
    "            if new_net_r.shape[0] != 0:       \n",
    "                median_frame_r_x = np.median(np.absolute(new_net_r[:, 0]))\n",
    "                median_frame_r_y = np.median(np.absolute(new_net_r[:, 1]))\n",
    "                median_list_r.append([median_frame_r_x,median_frame_r_y])\n",
    "                            \n",
    "            model_data.append(np.array([median_frame_l_x,median_frame_r_x, median_frame_l_y, median_frame_r_y])/input_dim)\n",
    "\n",
    "            plt.append(np.array([median_frame_l_x,median_frame_r_x, median_frame_l_y, median_frame_r_y])/input_dim)\n",
    "            labels.append(label)\n",
    "            model_labels.append(network_class_name)\n",
    "            \n",
    "            # Update the previous frame and previous points\n",
    "            old_gray = frame_gray.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "            \n",
    "            if np.array(median_list_r).shape[0] >= frames:                \n",
    "                median_l = np.median(median_list_l) \n",
    "                if median_l >= 0.09 and median_l <= 30:\n",
    "                    median_class_name_l = 1\n",
    "                else:\n",
    "                    median_class_name_l = 0\n",
    "                                                    \n",
    "                median_r = np.median(median_list_r) \n",
    "                if median_r >= 0.09 and median_r <= 30:\n",
    "                    median_class_name_r = 1\n",
    "                else:\n",
    "                    median_class_name_r = 0\n",
    "                    \n",
    "                label = 0   \n",
    "                if median_class_name_r == 1 and median_class_name_l == 1:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 0\n",
    "                    \n",
    "                predicted_labels_probabilities = model.predict(np.array(model_data),callbacks=callbacks)\n",
    "                network_class_name = np.argmax([np.median(np.array(predicted_labels_probabilities)[:,0]),\n",
    "                                                np.median(np.array(predicted_labels_probabilities)[:,1])])\n",
    "                break\n",
    "                \n",
    "    return plt, labels, model_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5458067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Возвращает данные о движении пикселей и состояния с порога\n",
    "def data(video_path):  \n",
    "    # Read the video \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "    # Parameters for ShiTomasi corner detection\n",
    "    feature_params = dict(maxCorners = maxCorners,\n",
    "                       qualityLevel = 0.07,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "    # Parameters for Lucas Kanade optical flow\n",
    "    lk_params = dict(winSize  = (15, 15),\n",
    "                  maxLevel = 3,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    plt = []\n",
    "    labels = []\n",
    "    median_class_name = ' '\n",
    "    label = 0\n",
    "    while True:       \n",
    "        # Take first frame and find corners in it\n",
    "        ret, old_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        old_frame = cv2.resize(old_frame, (input_dim, input_dim))        \n",
    "        \n",
    "        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)        \n",
    "                \n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "        # Create a mask image for drawing purposes\n",
    "        mask = np.zeros_like(old_frame)        \n",
    "        \n",
    "        median_list_l = []\n",
    "        median_list_r = []\n",
    "        \n",
    "        while True:\n",
    "            # Read new frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (input_dim, input_dim))\n",
    "            \n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # Calculate Optical Flow\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(\n",
    "                old_gray, frame_gray, p0, None, **lk_params\n",
    "            )\n",
    "            \n",
    "            if p1 is None:\n",
    "                break\n",
    "\n",
    "            # Select good points\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "            #print(good_new)\n",
    "            new_net_l = np.array([])\n",
    "            new_net_r = np.array([])\n",
    "            for i in range(len(good_new)):\n",
    "                if good_new[i][0] < input_dim/2:\n",
    "                    if len(new_net_l) == 0:\n",
    "                        new_net_l = np.array([good_new[i] - good_old[i]])\n",
    "                    else:\n",
    "                        new_net_l = np.append(new_net_l, [good_new[i] - good_old[i]], axis=0)\n",
    "                else:\n",
    "                    if len(new_net_r) == 0:\n",
    "                        new_net_r = np.array([good_new[i] - good_old[i]])\n",
    "                    else:\n",
    "                        new_net_r = np.append(new_net_r, [good_new[i] - good_old[i]], axis=0)\n",
    " \n",
    "            mask_med = np.zeros_like(old_frame)\n",
    "            if new_net_l.shape[0] != 0:\n",
    "                median_frame_l_x = np.median(np.absolute(new_net_l[:, 0]))\n",
    "                median_frame_l_y = np.median(np.absolute(new_net_l[:, 1]))\n",
    "                median_list_l.append([median_frame_l_x,median_frame_l_y])\n",
    "            else:\n",
    "                median_list_l.append([0,0])\n",
    "            \n",
    "            if new_net_r.shape[0] != 0:       \n",
    "                median_frame_r_x = np.median(np.absolute(new_net_r[:, 0]))\n",
    "                median_frame_r_y = np.median(np.absolute(new_net_r[:, 1]))\n",
    "                median_list_r.append([median_frame_r_x,median_frame_r_y])\n",
    "            else:\n",
    "                median_list_r.append([0,0])\n",
    "    \n",
    "            \n",
    "            plt.append(np.array([median_frame_l_x,median_frame_r_x, median_frame_l_y, median_frame_r_y])/input_dim)  \n",
    "            labels.append(np.array([label])) \n",
    "            # Update the previous frame and previous points\n",
    "            old_gray = frame_gray.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "            if np.array(median_list_r).shape[0] >= frames:                \n",
    "                median_l = np.median(median_list_l) \n",
    "                if median_l >= 0.09 and median_l <= 30:\n",
    "                    median_class_name_l = 1\n",
    "                else:\n",
    "                    median_class_name_l = 0\n",
    "                                                    \n",
    "                median_r = np.median(median_list_r) \n",
    "                if median_r >= 0.09 and median_r <= 30:\n",
    "                    median_class_name_r = 1\n",
    "                else:\n",
    "                    median_class_name_r = 0\n",
    "                    \n",
    "                label = 0   \n",
    "                if median_class_name_r == 1 and median_class_name_l == 1:\n",
    "                    median_class_name = classes_list[1]\n",
    "                    label = 1\n",
    "                else:\n",
    "                    median_class_name = classes_list[0]\n",
    "                    label = 0\n",
    "                break\n",
    "    return plt, labels\n",
    "    #video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a645f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN(units, input_dim, output_size): \n",
    "    model = keras.Sequential()\n",
    "    model.add(Input((4,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e8c233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RNN(units, output_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(units, return_sequences=True, input_shape=(4)))\n",
    "    model.add(layers.GRU(64))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(output_size))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37f43162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Early Stopping Callback\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15,\n",
    "                                        mode = 'max', restore_best_weights = True)\n",
    "\n",
    "model = get_CNN(8, 4, 2)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14a33af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ./vertical/Video-20-21-59-727-24-09-2021/Video-20-21-59-727 (S04R)\n",
      "2 ./vertical/Video-11-36-16-421-28-12-2020/Video-11-36-16-421 (PFAQ)\n",
      "3 ./vertical/Video-10-30-21-575-29-09-2021/Video-10-30-21-575 (UOM8)\n",
      "4 ./vertical/Video-13-16-32-822-10-09-2021/Video-13-16-32-822 (UQ1Q)\n",
      "5 ./vertical/Video-12-23-51-640-02-10-2021/Video-12-23-51-640 (WLWK)\n",
      "6 ./vertical/Video-13-20-30-695-15-11-2021/Video-13-20-30-695 (VLFO)\n",
      "7 ./vertical/Video-20-45-56-152-24-10-2021/Video-20-45-56-152 (6M38)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'median_frame_r_x' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(j, i)\n\u001b[0;32m----> 7\u001b[0m x_i, y_i \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_i\n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mdata\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     median_list_r\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 89\u001b[0m plt\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([median_frame_l_x,\u001b[43mmedian_frame_r_x\u001b[49m, median_frame_l_y, median_frame_r_y])\u001b[38;5;241m/\u001b[39minput_dim)  \n\u001b[1;32m     90\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([label])) \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Update the previous frame and previous points\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'median_frame_r_x' referenced before assignment"
     ]
    }
   ],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "j = 0 \n",
    "for i in vertical:  \n",
    "    j += 1\n",
    "    print(j, i)\n",
    "    x_i, y_i = data(i + '.mp4')\n",
    "    if j == 1:\n",
    "        x = x_i\n",
    "        y = y_i\n",
    "    else:\n",
    "        x = np.concatenate((x, x_i), 0)\n",
    "        y = np.concatenate((y, y_i), 0)\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41c5d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x', x)\n",
    "np.save('y', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d53a8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = np.load('./processed/val_x.npy')\n",
    "val_y = np.load('./processed/val_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b31ece95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ver = np.load('./processed/ver/x.npy')\n",
    "y_ver = np.load('./processed/ver/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e358e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hor = np.load('./processed/hor/x.npy')\n",
    "y_hor = np.load('./processed/hor/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "323a4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_ver, x_hor), 0)\n",
    "y = np.concatenate((y_ver, y_hor), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be31717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fda70056c70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./weights/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5eee3977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "552/556 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9397\n",
      "Epoch 1: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1639 - accuracy: 0.9397 - val_loss: 0.4332 - val_accuracy: 0.7796\n",
      "Epoch 2/15\n",
      "543/556 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9399\n",
      "Epoch 2: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1639 - accuracy: 0.9398 - val_loss: 0.4239 - val_accuracy: 0.8460\n",
      "Epoch 3/15\n",
      "546/556 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9407\n",
      "Epoch 3: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1634 - accuracy: 0.9406 - val_loss: 0.1768 - val_accuracy: 0.9523\n",
      "Epoch 4/15\n",
      "543/556 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9405\n",
      "Epoch 4: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1629 - accuracy: 0.9404 - val_loss: 0.5075 - val_accuracy: 0.4758\n",
      "Epoch 5/15\n",
      "549/556 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9402\n",
      "Epoch 5: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1631 - accuracy: 0.9402 - val_loss: 0.3829 - val_accuracy: 0.9483\n",
      "Epoch 6/15\n",
      "546/556 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9407\n",
      "Epoch 6: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1632 - accuracy: 0.9407 - val_loss: 0.3276 - val_accuracy: 0.8636\n",
      "Epoch 7/15\n",
      "548/556 [============================>.] - ETA: 0s - loss: 0.1637 - accuracy: 0.9401\n",
      "Epoch 7: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1636 - accuracy: 0.9402 - val_loss: 0.3165 - val_accuracy: 0.8817\n",
      "Epoch 8/15\n",
      "548/556 [============================>.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9402\n",
      "Epoch 8: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1636 - accuracy: 0.9402 - val_loss: 0.2198 - val_accuracy: 0.9588\n",
      "Epoch 9/15\n",
      "543/556 [============================>.] - ETA: 0s - loss: 0.1637 - accuracy: 0.9404\n",
      "Epoch 9: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1637 - accuracy: 0.9405 - val_loss: 0.3743 - val_accuracy: 0.9320\n",
      "Epoch 10/15\n",
      "546/556 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9404\n",
      "Epoch 10: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1635 - accuracy: 0.9403 - val_loss: 0.6626 - val_accuracy: 0.4766\n",
      "Epoch 11/15\n",
      "552/556 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9402\n",
      "Epoch 11: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1625 - accuracy: 0.9403 - val_loss: 0.2592 - val_accuracy: 0.9408\n",
      "Epoch 12/15\n",
      "555/556 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9397\n",
      "Epoch 12: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1648 - accuracy: 0.9398 - val_loss: 0.1916 - val_accuracy: 0.9347\n",
      "Epoch 13/15\n",
      "544/556 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9406\n",
      "Epoch 13: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1630 - accuracy: 0.9406 - val_loss: 0.3934 - val_accuracy: 0.8315\n",
      "Epoch 14/15\n",
      "555/556 [============================>.] - ETA: 0s - loss: 0.1632 - accuracy: 0.9402\n",
      "Epoch 14: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1632 - accuracy: 0.9402 - val_loss: 0.3817 - val_accuracy: 0.8913\n",
      "Epoch 15/15\n",
      "555/556 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9405\n",
      "Epoch 15: val_accuracy did not improve from 0.97921\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.1628 - accuracy: 0.9405 - val_loss: 0.1601 - val_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "        x, y, batch_size=1000, epochs=15,\n",
    "        callbacks = callbacks,\n",
    "        validation_data  = (val_x, val_y),\n",
    "        shuffle = True \n",
    ")\n",
    "\n",
    "model.save_weights('./weights/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = data('./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52727030",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('val_x', val_x)\n",
    "np.save('val_y', val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a52d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = np.array(val_x)\n",
    "val_y = np.array(val_y)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d9d2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = data('./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f40707",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(tx.shape[0]), ty)\n",
    "plt.plot(range(tx.shape[0]), ty/9)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e7835a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p2, l2 \u001b[38;5;241m=\u001b[39m \u001b[43mreal_time\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mreal_time\u001b[0;34m(video_path, model, callbacks)\u001b[0m\n\u001b[1;32m    141\u001b[0m     median_class_name \u001b[38;5;241m=\u001b[39m classes_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    142\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 144\u001b[0m predicted_labels_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m network_class_name \u001b[38;5;241m=\u001b[39m classes_list[np\u001b[38;5;241m.\u001b[39margmax([np\u001b[38;5;241m.\u001b[39mmedian(np\u001b[38;5;241m.\u001b[39marray(predicted_labels_probabilities)[:,\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    146\u001b[0m                                             np\u001b[38;5;241m.\u001b[39mmedian(np\u001b[38;5;241m.\u001b[39marray(predicted_labels_probabilities)[:,\u001b[38;5;241m1\u001b[39m])])]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frames):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1951\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1944\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1949\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 1951\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py:1399\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1148\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py:326\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n\u001b[1;32m    324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m--> 326\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2060\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2027\u001b[0m   \u001b[38;5;124;03m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[1;32m   2028\u001b[0m \n\u001b[1;32m   2029\u001b[0m \u001b[38;5;124;03m  The type signature is:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2060\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:5279\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m   5277\u001b[0m \u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m   5278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m-> 5279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[1;32m   5282\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   5283\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5284\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_get_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3070\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3062\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   3063\u001b[0m \n\u001b[1;32m   3064\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3070\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3072\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3036\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 3036\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   3038\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   3039\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3292\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3288\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3289\u001b[0m       args, kwargs, flat_args, filtered_flat_args)\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd_call_context(cache_key\u001b[38;5;241m.\u001b[39mcall_context)\n\u001b[0;32m-> 3292\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                          graph_function)\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3125\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3127\u001b[0m ]\n\u001b[1;32m   3128\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3129\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3130\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3142\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3143\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3145\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1044\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1042\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op_return_value, ops\u001b[38;5;241m.\u001b[39mTensor), op_return_value\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mFuncGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_by_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_graph, FuncGraph)\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_control_dependencies:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:195\u001b[0m, in \u001b[0;36mFuncGraph.__init__\u001b[0;34m(self, name, collections, capture_by_value, structured_input_signature, structured_outputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, capture_by_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    170\u001b[0m              structured_input_signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, structured_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;124;03m\"\"\"Construct a new FuncGraph.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m  The graph will inherit its graph key, collections, seed, and distribution\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m      information.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3192\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduced_shape_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   3190\u001b[0m \u001b[38;5;66;03m# TODO(skyewm): fold as much of the above as possible into the C\u001b[39;00m\n\u001b[1;32m   3191\u001b[0m \u001b[38;5;66;03m# implementation\u001b[39;00m\n\u001b[0;32m-> 3192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scoped_c_graph \u001b[38;5;241m=\u001b[39m \u001b[43mc_api_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScopedTFGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3193\u001b[0m \u001b[38;5;66;03m# The C API requires all ops to have shape functions. Disable this\u001b[39;00m\n\u001b[1;32m   3194\u001b[0m \u001b[38;5;66;03m# requirement (many custom ops do not have shape functions, and we don't\u001b[39;00m\n\u001b[1;32m   3195\u001b[0m \u001b[38;5;66;03m# want to break these existing cases).\u001b[39;00m\n\u001b[1;32m   3196\u001b[0m pywrap_tf_session\u001b[38;5;241m.\u001b[39mSetRequireShapeInferenceFns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_graph, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/c_api_util.py:46\u001b[0m, in \u001b[0;36mScopedTFGraph.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[43mc_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_NewGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;66;03m# Note: when we're destructing the global context (i.e when the process is\u001b[39;00m\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;66;03m# terminating) we may have already deleted other modules. By capturing the\u001b[39;00m\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;66;03m# DeleteGraph function here, we retain the ability to cleanly destroy the\u001b[39;00m\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;66;03m# graph at shutdown, which satisfies leak checkers.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeleter \u001b[38;5;241m=\u001b[39m c_api\u001b[38;5;241m.\u001b[39mTF_DeleteGraph\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p2, l2 = real_time('./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4', model, callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fee6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_4, labels, model_labels = data_model('./horizontal/Video-15-47-49-515-31-03-2022/Video-15-47-49-515 (4IIC).mp4', model, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93bf9597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = np.array(med_4).shape[0]\n",
    "plt.plot(range(num), med_4)\n",
    "plt.plot(range(num), np.array(labels)*9)\n",
    "#plt.plot(range(num), np.array(model_labels)*9)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(num), med_4)\n",
    "#plt.plot(range(num), np.array(labels)*9)\n",
    "plt.plot(range(num), np.array(model_labels)*9)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
